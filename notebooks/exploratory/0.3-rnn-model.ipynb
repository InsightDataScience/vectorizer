{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('/Users/pujaarajan/Documents/GitHub/nlp_fib/data/raw/enron_05_17_2015_with_labels_v2_100K_chunk_1_of_6.csv', usecols= [*range(2, 6),13], dtype={13:str})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Bidirectional Language Model\n",
    "## https://gist.github.com/tokestermw/912042a85a1d53169c2dc7253dca55f6#file-birnnlm_pytorch-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text = ['BOS', 'How', 'are', 'you', 'EOS']\n",
    "seq_len = len(text)\n",
    "batch_size = 1\n",
    "embedding_size = 1\n",
    "hidden_size = 1\n",
    "output_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0952]],\n",
      "\n",
      "        [[ 0.4962]],\n",
      "\n",
      "        [[ 0.4910]],\n",
      "\n",
      "        [[-1.0986]],\n",
      "\n",
      "        [[ 0.3712]]])\n",
      "RNN(1, 1, bidirectional=True)\n",
      "Linear(in_features=2, out_features=1, bias=True)\n",
      "tensor([[[ 0.4962]],\n",
      "\n",
      "        [[ 0.4910]],\n",
      "\n",
      "        [[-1.0986]]])\n"
     ]
    }
   ],
   "source": [
    "random_input = Variable(\n",
    "    torch.FloatTensor(seq_len, batch_size, embedding_size).normal_(), requires_grad=False)\n",
    "\n",
    "print(random_input)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN(1, 1, bidirectional=True)\n",
      "Linear(in_features=2, out_features=1, bias=True)\n",
      "tensor([[[ 0.4962]],\n",
      "\n",
      "        [[ 0.4910]],\n",
      "\n",
      "        [[-1.0986]]])\n"
     ]
    }
   ],
   "source": [
    "bi_rnn = torch.nn.RNN(input_size=embedding_size, hidden_size=hidden_size, num_layers=1, batch_first=False, bidirectional=True)\n",
    "\n",
    "print (bi_rnn)\n",
    "\n",
    "bi_output, bi_hidden = bi_rnn(random_input)\n",
    "\n",
    "# stagger\n",
    "forward_output, backward_output = bi_output[:-2, :, :hidden_size], bi_output[2:, :, hidden_size:]\n",
    "staggered_output = torch.cat((forward_output, backward_output), dim=-1)\n",
    "\n",
    "linear = nn.Linear(hidden_size * 2, output_size)\n",
    "\n",
    "print(linear)\n",
    "\n",
    "# only predict on words\n",
    "labels = random_input[1:-1]\n",
    "\n",
    "print (labels)\n",
    "\n",
    "# for language models, use cross-entropy :)\n",
    "loss = nn.MSELoss()\n",
    "output = loss(linear(staggered_output), labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Bidirectional RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.8969]],\n",
       "\n",
       "        [[-1.6470]],\n",
       "\n",
       "        [[-1.6635]],\n",
       "\n",
       "        [[-0.6729]],\n",
       "\n",
       "        [[-0.2843]],\n",
       "\n",
       "        [[ 0.5671]],\n",
       "\n",
       "        [[-0.0598]],\n",
       "\n",
       "        [[-0.6298]]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch, torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "text = ['BOS', 'How', 'are', 'you','EOS', 'BOS', 'Hi', 'Bye' 'EOS']\n",
    "\n",
    "seq_len = len(text)\n",
    "batch_size = 1 # Change this\n",
    "embedding_size = 1 # Could be the bumber of words you start with to predict next or \n",
    "hidden_size = 1\n",
    "output_size = 1\n",
    "\n",
    "#\n",
    "     torch.FloatTensor(seq_len, batch_size, embedding_size).normal_(), requires_grad=False)\n",
    "\n",
    "random_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.0742, -0.2596]],\n",
      "\n",
      "        [[-0.8175, -0.4755]],\n",
      "\n",
      "        [[-0.7459, -0.4198]],\n",
      "\n",
      "        [[-0.5891, -0.3133]],\n",
      "\n",
      "        [[-0.5346, -0.2494]],\n",
      "\n",
      "        [[-0.3344, -0.1926]],\n",
      "\n",
      "        [[-0.5318, -0.2071]],\n",
      "\n",
      "        [[-0.6156, -0.1532]]], grad_fn=<CatBackward>)\n",
      "tensor([[[-0.6156]],\n",
      "\n",
      "        [[-0.2596]]], grad_fn=<StackBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Pretrained model\n",
    "bi_rnn = torch.nn.RNN(\n",
    "    input_size=embedding_size, hidden_size=hidden_size, num_layers=1, batch_first=False, bidirectional=True)\n",
    "\n",
    "bi_output, bi_hidden = bi_rnn(random_input)\n",
    "\n",
    "print(bi_output)\n",
    "\n",
    "print(bi_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.9471]],\n",
      "\n",
      "        [[-0.8963]],\n",
      "\n",
      "        [[-0.0850]]], grad_fn=<SliceBackward>)\n",
      "tensor([[[-0.7963]],\n",
      "\n",
      "        [[-0.9743]],\n",
      "\n",
      "        [[-0.9580]]], grad_fn=<SliceBackward>)\n"
     ]
    }
   ],
   "source": [
    "# stagger\n",
    "forward_output, backward_output = bi_output[:-2, :, :hidden_size], bi_output[2:, :, hidden_size:]\n",
    "\n",
    "print(forward_output)\n",
    "print(backward_output)\n",
    "\n",
    "#forward_output - first 3 word features BOS How Are\n",
    "#backward_output has the last word features Are You EOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.9471, -0.7963]],\n",
       "\n",
       "        [[-0.8963, -0.9743]],\n",
       "\n",
       "        [[-0.0850, -0.9580]]], grad_fn=<CatBackward>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "staggered_output = torch.cat((forward_output, backward_output), dim=-1)\n",
    "\n",
    "staggered_output \n",
    "#first column forward output and second column is backward output\n",
    "#first column - BOS how are\n",
    "#second column - are you EOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=2, out_features=1, bias=True)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear = nn.Linear(hidden_size * 2, output_size)\n",
    "\n",
    "linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# only predict on words\n",
    "original_words = random_input[1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0421]],\n",
       "\n",
       "        [[-1.4417]],\n",
       "\n",
       "        [[ 0.0216]]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MSELoss()"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for language models, use cross-entropy :)\n",
    "loss = nn.MSELoss()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.1693]],\n",
       "\n",
       "        [[-1.2597]],\n",
       "\n",
       "        [[-1.1365]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear(staggered_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8816, grad_fn=<MseLossBackward>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = loss(linear(staggered_output), original_words)\n",
    "output\n",
    "# calculate mean absolute error bw each element in input x and output y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, label_size, batch_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True)\n",
    "        self.hidden2label = nn.Linear(hidden_dim * 2, label_size) # Ã„: hidden_dim * 2 for bidirectional\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "    def init_hidden(self):\n",
    "\n",
    "        h0 = Variable(torch.zeros(1 * 2, self.batch_size, self.hidden_dim))\n",
    "        c0 = Variable(torch.zeros(1 * 2, self.batch_size, self.hidden_dim))\n",
    "        return h0, c0\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        x = embeds.view(len(sentence), self.batch_size, -1)\n",
    "        lstm_out, self.hidden = self.lstm(x, self.hidden)\n",
    "        y = self.hidden2label(lstm_out[-1])\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
