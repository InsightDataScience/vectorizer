{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/pujaarajan/Documents/GitHub/nlp_fib/data/raw/enron_05_17_2015_with_labels_v2_100K_chunk_1_of_6.csv', usecols= [*range(2, 6),13], dtype={13:str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>From</th>\n",
       "      <th>To</th>\n",
       "      <th>Subject</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2001-05-14 23:39:00</td>\n",
       "      <td>frozenset({'phillip.allen@enron.com'})</td>\n",
       "      <td>frozenset({'tim.belden@enron.com'})</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Here is our forecast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2001-05-04 20:51:00</td>\n",
       "      <td>frozenset({'phillip.allen@enron.com'})</td>\n",
       "      <td>frozenset({'john.lavorato@enron.com'})</td>\n",
       "      <td>Re:</td>\n",
       "      <td>Traveling to have a business meeting takes the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000-10-18 10:00:00</td>\n",
       "      <td>frozenset({'phillip.allen@enron.com'})</td>\n",
       "      <td>frozenset({'leah.arsdall@enron.com'})</td>\n",
       "      <td>Re: test</td>\n",
       "      <td>test successful. way to go!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2000-10-23 13:13:00</td>\n",
       "      <td>frozenset({'phillip.allen@enron.com'})</td>\n",
       "      <td>frozenset({'randall.gay@enron.com'})</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Randy, Can you send me a schedule of the salar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2000-08-31 12:07:00</td>\n",
       "      <td>frozenset({'phillip.allen@enron.com'})</td>\n",
       "      <td>frozenset({'greg.piper@enron.com'})</td>\n",
       "      <td>Re: Hello</td>\n",
       "      <td>Let's shoot for Tuesday at 11:45.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Date                                    From  \\\n",
       "0  2001-05-14 23:39:00  frozenset({'phillip.allen@enron.com'})   \n",
       "1  2001-05-04 20:51:00  frozenset({'phillip.allen@enron.com'})   \n",
       "2  2000-10-18 10:00:00  frozenset({'phillip.allen@enron.com'})   \n",
       "3  2000-10-23 13:13:00  frozenset({'phillip.allen@enron.com'})   \n",
       "4  2000-08-31 12:07:00  frozenset({'phillip.allen@enron.com'})   \n",
       "\n",
       "                                       To    Subject  \\\n",
       "0     frozenset({'tim.belden@enron.com'})        NaN   \n",
       "1  frozenset({'john.lavorato@enron.com'})        Re:   \n",
       "2   frozenset({'leah.arsdall@enron.com'})   Re: test   \n",
       "3    frozenset({'randall.gay@enron.com'})        NaN   \n",
       "4     frozenset({'greg.piper@enron.com'})  Re: Hello   \n",
       "\n",
       "                                             content  \n",
       "0                               Here is our forecast  \n",
       "1  Traveling to have a business meeting takes the...  \n",
       "2                      test successful. way to go!!!  \n",
       "3  Randy, Can you send me a schedule of the salar...  \n",
       "4                  Let's shoot for Tuesday at 11:45.  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tagger', 'preprocesser', 'parser', 'ner']\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en')\n",
    "\n",
    "#punctuations = string.punctuation\n",
    "\n",
    "def preprocess_text(doc):\n",
    "    \"\"\" Preprocess text. Tokenize, lowercase, and remove punctuation and stopwords \"\"\"\n",
    "    stopwords = spacy.lang.en.STOP_WORDS\n",
    "    # Tokenize, remove punctuation, symbols (#), stopwords\n",
    "    doc = [tok.text for tok in doc if (tok.text not in stopwords and tok.pos_ != \"PUNCT\" and tok.pos_ != \"SYM\" and tok.pos_ != \"X\")]\n",
    "    # Lowercase tokens\n",
    "    doc = [tok.lower() for tok in doc]\n",
    "    doc = ' '.join(doc)\n",
    "    return nlp.make_doc(doc)\n",
    "\n",
    "nlp.add_pipe(preprocess_text, name='preprocesser', after='tagger')\n",
    "print(nlp.pipe_names) # ['tagger', 'preprocesser','parser', 'ner']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spacy_tokenized_emails = []\n",
    "for email in sample_emails:\n",
    "    sentence_tokens = []\n",
    "    doc = nlp(email[0])\n",
    "    for token in doc:\n",
    "        sentence_tokens.append(token.text)\n",
    "    spacy_tokenized_emails.append(sentence_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "text = ['BOS', 'How', 'are', 'you', 'EOS']\n",
    "seq_len = len(text)\n",
    "batch_size = 1\n",
    "embedding_size = 1\n",
    "hidden_size = 1\n",
    "output_size = 1\n",
    "\n",
    "random_input = Variable(\n",
    "    torch.FloatTensor(seq_len, batch_size, embedding_size).normal_(), requires_grad=False)\n",
    "\n",
    "bi_rnn = torch.nn.RNN(\n",
    "    input_size=embedding_size, hidden_size=hidden_size, num_layers=1, batch_first=False, bidirectional=True)\n",
    "\n",
    "bi_output, bi_hidden = bi_rnn(random_input)\n",
    "\n",
    "# stagger\n",
    "forward_output, backward_output = bi_output[:-2, :, :hidden_size], bi_output[2:, :, hidden_size:]\n",
    "staggered_output = torch.cat((forward_output, backward_output), dim=-1)\n",
    "\n",
    "linear = nn.Linear(hidden_size * 2, output_size)\n",
    "\n",
    "# only predict on words\n",
    "labels = random_input[1:-1]\n",
    "\n",
    "# for language models, use cross-entropy :)\n",
    "loss = nn.MSELoss()\n",
    "output = loss(linear(staggered_output), labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.9892]],\n",
       "\n",
       "        [[ 0.6772]],\n",
       "\n",
       "        [[-2.7760]]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Here', 'is', 'our', 'forecast'], ['Traveling', 'to', 'have', 'a', 'business', 'meeting', 'takes', 'the', 'fun', 'out', 'of', 'the', 'trip', '.', 'Especially', 'if', 'you', 'have', 'to', 'prepare', 'a', 'presentation', '.', 'I', 'would', 'suggest', 'holding', 'the', 'business', 'plan', 'meetings', 'here', 'then', 'take', 'a', 'trip', 'without', 'any', 'formal', 'business', 'meetings', '.', 'I', 'would', 'even', 'try', 'and', 'get', 'some', 'honest', 'opinions', 'on', 'whether', 'a', 'trip', 'is', 'even', 'desired', 'or', 'necessary', '.', 'As', 'far', 'as', 'the', 'business', 'meetings', ',', 'I', 'think', 'it', 'would', 'be', 'more', 'productive', 'to', 'try', 'and', 'stimulate', 'discussions', 'across', 'the', 'different', 'groups', 'about', 'what', 'is', 'working', 'and', 'what', 'is', 'not', '.', 'Too', 'often', 'the', 'presenter', 'speaks', 'and', 'the', 'others', 'are', 'quiet', 'just', 'waiting', 'for', 'their', 'turn', '.', 'The', 'meetings', 'might', 'be', 'better', 'if', 'held', 'in', 'a', 'round', 'table', 'discussion', 'format', '.', 'My', 'suggestion', 'for', 'where', 'to', 'go', 'is', 'Austin', '.', 'Play', 'golf', 'and', 'rent', 'a', 'ski', 'boat', 'and', 'jet', 'ski', \"'s\", '.', 'Flying', 'somewhere', 'takes', 'too', 'much', 'time', '.'], ['test', 'successful', '.', 'way', 'to', 'go', '!', '!', '!'], ['Randy', ',', 'Can', 'you', 'send', 'me', 'a', 'schedule', 'of', 'the', 'salary', 'and', 'level', 'of', 'everyone', 'in', 'the', 'scheduling', 'group', '.', 'Plus', 'your', 'thoughts', 'on', 'any', 'changes', 'that', 'need', 'to', 'be', 'made', '.', '(', 'Patti', 'S', 'for', 'example', ')', 'Phillip'], ['Let', \"'s\", 'shoot', 'for', 'Tuesday', 'at', '11:45', '.']]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "tokenized_emails = []\n",
    "for email in sample_emails:\n",
    "    tokens = word_tokenize(email[0])\n",
    "    tokenized_emails.append(tokens)\n",
    "print(tokenized_emails[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
